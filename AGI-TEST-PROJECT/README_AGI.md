# Neural Network Primitives: Foundation Experiments

> **Validation of basic neural network capabilities in Charl**
> *Testing tensor operations, layers, and learning primitives*

## Overview

This project validates Charl's capability for neural network development through incremental experiments testing core primitives: tensor operations, linear layers, activation functions, backpropagation, and simple learning tasks.

**Important**: This is NOT an AGI project. These are foundational experiments demonstrating that Charl can implement basic neural network operations correctly.

## Experiments

| Level | Test | Parameters | Accuracy | Status |
|-------|------|------------|----------|--------|
| 1 | Tensor operations | ~4 | 100% | âœ… |
| 2 | Linear layers | ~13 | 100% | âœ… |
| 3 | Activation functions | ~11 | 100% | âœ… |
| 4 | Multi-layer networks | ~60 | 100% | âœ… |
| 5 | Basic learning | ~100 | 75% | âœ… |
| 6 | Gradient computation | ~200 | 100% | âœ… |
| 7 | Simple classification | ~300 | 100% | âœ… |
| 8 | Multi-task learning | ~500 | 100% | âœ… |

## Quick Start

```bash
# Run validation tests
./target/release/charl run test_MINIMAL_REASONER.ch
./target/release/charl run test_COMPOSITIONAL_REASONER.ch
./target/release/charl run test_ABSTRACT_REASONER.ch
./target/release/charl run test_META_REASONER.ch
./target/release/charl run test_TRANSFER_LEARNER.ch
./target/release/charl run test_CAUSAL_REASONER.ch
./target/release/charl run test_PLANNING_REASONER.ch
./target/release/charl run test_SELF_REFLECTION_AGI.ch
```

## What Was Validated

- âœ… **Tensor operations**: Matrix multiplication, broadcasting, reshaping
- âœ… **Neural layers**: Linear, embedding, activations (ReLU, Softmax)
- âœ… **Backpropagation**: Gradient computation and parameter updates
- âœ… **Training loops**: Forward pass, loss calculation, backward pass
- âœ… **Simple learning**: Convergence on toy datasets

## Documentation

See [AGI_JOURNEY.md](./AGI_JOURNEY.md) for:
- Detailed experiment descriptions
- Implementation notes
- Results and metrics
- Technical specifications

## Purpose

This project serves as **proof-of-concept** that Charl can:
1. Implement tensor operations correctly
2. Build neural network layers (linear, embedding, etc.)
3. Compute gradients via backpropagation
4. Train simple models that converge

These primitives form the foundation for more advanced architectures like the Mixture of Experts system in AGI_PROJECT_III.

## Progression

```
Level 1: Tensors    â†’  Level 2: Layers     â†’  Level 3: Activations
  (~4 params)           (~13 params)            (~11 params)
      â†“                      â†“                      â†“
Level 4: Networks   â†’  Level 5: Learning   â†’  Level 6: Gradients
  (~60 params)          (~100 params)           (~200 params)
      â†“                      â†“                      â†“
Level 7: Classification  â†’  Level 8: Multi-task
  (~300 params)              (~500 params) âœ…
```

## What This Is NOT

- âŒ NOT AGI or anything close to AGI
- âŒ NOT comparable to GPT-4 or any large language model
- âŒ NOT a general-purpose AI system
- âŒ NOT production-ready models

## What This IS

- âœ… Validation of neural network primitives in Charl
- âœ… Foundation for building more complex architectures
- âœ… Proof that Charl's tensor operations work correctly
- âœ… Test suite for gradient computation and training

## Links

- ğŸ“– [Complete Documentation](./AGI_JOURNEY.md)
- ğŸ’» [Source Code](./test_SELF_REFLECTION_AGI.ch)
- ğŸ”¬ [AGI_PROJECT_III](../AGI_PROJECT_III/) - Actual research project using these primitives

## Citation

```bibtex
@misc{charl-primitives-2025,
  title={Neural Network Primitives Validation in Charl},
  author={Charl Development Team},
  year={2025},
  note={Foundation experiments for neural network capabilities}
}
```

## License

MIT License - See [LICENSE](./LICENSE) for details.

---

<div align="center">

**Validating Fundamentals**

*Building blocks for neural architecture research*

</div>
