# Attention Mechanisms - COMPLETADO âœ…

## Resumen Ejecutivo

Hemos implementado los componentes fundamentales de Attention para preparar la base de **Fase 14: Neuro-Symbolic Integration**.

**Fecha:** 2025-11-04
**Tests:** 34 nuevos (100% pasando)
**Total Charl:** 248 tests pasando
**CÃ³digo:** ~1,680 lÃ­neas nuevas

---

## ğŸ¯ Componentes Implementados

### 1. Scaled Dot-Product Attention âœ…
**Archivo:** `src/attention/scaled_attention.rs` (~300 lÃ­neas)

ImplementaciÃ³n completa del mecanismo fundamental de attention:

```
Attention(Q, K, V) = softmax(QK^T / sqrt(d_k))V
```

**CaracterÃ­sticas:**
- âœ… Matrix multiplication optimizado
- âœ… Scaling por sqrt(d_k) para estabilidad numÃ©rica
- âœ… Softmax con numerical stability (max subtraction)
- âœ… Attention masking (para causal attention)
- âœ… Soporte para diferentes seq_len en Q y K (encoder-decoder)
- âœ… 5 tests comprehensivos

**Ejemplo de uso:**
```rust
let attention = ScaledDotProductAttention::new(64, 0.1);
let (output, weights) = attention.forward(
    query, key, value,
    (batch, seq_len_q, d_k),
    (batch, seq_len_k, d_k),
    (batch, seq_len_v, d_v),
    Some(mask)
)?;
```

### 2. Self-Attention âœ…
**Archivo:** `src/attention/self_attention.rs` (~280 lÃ­neas)

Self-attention donde Q, K, V vienen de la misma fuente:

**CaracterÃ­sticas:**
- âœ… Proyecciones lineales aprendidas (W_Q, W_K, W_V, W_O)
- âœ… Forward pass completo
- âœ… Soporte para attention masks
- âœ… 5 tests incluyendo causal masking

**Ejemplo:**
```rust
let self_attn = SelfAttention::new(128, 64, 64, 0.1);
let (output, weights) = self_attn.forward(
    input,
    (batch, seq_len, d_model),
    Some(causal_mask)
)?;
```

### 3. Multi-Head Attention âœ…
**Archivo:** `src/attention/multi_head.rs` (~520 lÃ­neas)

El componente clave de Transformers - attention en mÃºltiples subspacios:

```
MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O
donde head_i = Attention(QW^Q_i, KW^K_i, VW^V_i)
```

**CaracterÃ­sticas:**
- âœ… Splitting en mÃºltiples heads (d_model = num_heads * d_k)
- âœ… Attention paralelo en cada head
- âœ… ConcatenaciÃ³n de outputs
- âœ… ProyecciÃ³n final
- âœ… Soporte para diferentes seq_len (encoder-decoder attention)
- âœ… 8 tests incluyendo edge cases

**Ejemplo:**
```rust
let mha = MultiHeadAttention::new(512, 8, 0.1)?;  // 8 heads de 64 dims
let (output, weights) = mha.forward(
    query, key, value,
    (batch, seq_len_q, 512),
    (batch, seq_len_k, 512),
    (batch, seq_len_k, 512),
    Some(mask)
)?;
```

### 4. Layer Normalization âœ…
**Archivo:** `src/attention/layer_norm.rs` (~363 lÃ­neas)

NormalizaciÃ³n por capas - crÃ­tico para estabilidad de entrenamiento:

**CaracterÃ­sticas:**
- âœ… NormalizaciÃ³n across feature dimension
- âœ… Learnable scale (gamma) y shift (beta)
- âœ… Numerical stability (epsilon)
- âœ… 12 tests comprehensivos

**Ejemplo:**
```rust
let ln = LayerNorm::new(512, 1e-5);
let normalized = ln.forward(
    input,
    (batch, seq_len, features)
)?;
// Output: mean â‰ˆ 0, std â‰ˆ 1 across features
```

### 5. ConfiguraciÃ³n y Utilidades âœ…
**Archivo:** `src/attention/mod.rs` (~150 lÃ­neas)

**CaracterÃ­sticas:**
- âœ… AttentionConfig con presets (transformer_base, small, large)
- âœ… ValidaciÃ³n automÃ¡tica de configuraciones
- âœ… 4 tests de configuraciÃ³n

**Presets disponibles:**
```rust
// Transformer Base (512 dims, 8 heads)
let config = AttentionConfig::transformer_base();

// Small (128 dims, 4 heads) - para testing
let config = AttentionConfig::small();

// Large (1024 dims, 16 heads)
let config = AttentionConfig::transformer_large();
```

---

## ğŸ“Š EstadÃ­sticas

### Tests:
```
Attention Module: 34 tests
â”œâ”€ Scaled Attention: 5 tests
â”œâ”€ Self-Attention: 5 tests
â”œâ”€ Multi-Head Attention: 8 tests
â”œâ”€ Layer Normalization: 12 tests
â””â”€ Configuration: 4 tests

Total Charl: 248 tests (214 previos + 34 nuevos)
âœ… 100% passing
```

### CÃ³digo:
```
LÃ­neas por mÃ³dulo:
â”œâ”€ scaled_attention.rs: ~300 lÃ­neas
â”œâ”€ self_attention.rs: ~280 lÃ­neas
â”œâ”€ multi_head.rs: ~520 lÃ­neas
â”œâ”€ layer_norm.rs: ~363 lÃ­neas
â”œâ”€ mod.rs: ~150 lÃ­neas
â””â”€ stubs (positional_encoding): ~15 lÃ­neas

Total nuevo: ~1,628 lÃ­neas
```

### ParÃ¡metros:
```
Multi-Head Attention (d_model=512, heads=8):
â”œâ”€ W_Q: 512 x 512 = 262,144 params
â”œâ”€ W_K: 512 x 512 = 262,144 params
â”œâ”€ W_V: 512 x 512 = 262,144 params
â”œâ”€ W_O: 512 x 512 = 262,144 params
â””â”€ Total: 1,048,576 params (1M params)
```

---

## ğŸ”§ CaracterÃ­sticas TÃ©cnicas

### 1. Numerical Stability
```rust
// Softmax con max subtraction para estabilidad
let max_score = scores.iter().fold(f64::NEG_INFINITY, |a, &b| a.max(b));
let exp_score = (score - max_score).exp();
```

### 2. Flexible Sequence Lengths
```rust
// Q y K pueden tener diferentes longitudes
// Ãštil para encoder-decoder attention
let (output, _) = attention.forward(
    query,  // (batch, 10, d_k)  <- decoder
    key,    // (batch, 50, d_k)  <- encoder
    value,  // (batch, 50, d_v)
    ...
)?;
```

### 3. Attention Masking
```rust
// Causal mask (para autoregressive models)
let mut mask = vec![0.0; seq_len * seq_len];
for i in 0..seq_len {
    for j in 0..=i {
        mask[i * seq_len + j] = 1.0;  // Permite atender a posiciones <= i
    }
}
```

### 4. Multi-Head Parallel Processing
```rust
// Divide d_model en num_heads
// Cada head opera independientemente
// Luego concatena y proyecta
for h in 0..num_heads {
    let head_output = attention.forward(...)?;
    all_outputs.push(head_output);
}
let concat = concatenate_heads(&all_outputs)?;
let output = linear_projection(&concat, &w_o)?;
```

---

## ğŸš€ Uso en Neuro-Symbolic (Fase 14)

### 1. Knowledge Graph Attention
```rust
// Attention sobre nodos de knowledge graph
let graph_mha = MultiHeadAttention::new(256, 4, 0.1)?;
let node_embeddings = ...; // Embeddings de nodos
let (attended, weights) = graph_mha.forward(
    node_embeddings,  // Query: nodo actual
    node_embeddings,  // Key/Value: todos los nodos
    node_embeddings,
    ...
)?;
// weights[i][j] = quÃ© tan relacionados estÃ¡n nodos i y j
```

### 2. Hybrid Neural-Symbolic Layers
```rust
// Neural â†’ Attention â†’ Symbolic â†’ Neural
struct SymbolicAttentionLayer {
    neural_encoder: DenseLayer,
    attention: MultiHeadAttention,
    logic_rules: Vec<LogicRule>,
    neural_decoder: DenseLayer,
}

fn forward(x: Tensor) -> Tensor {
    let encoded = neural_encoder.forward(x);
    let (attended, _) = attention.forward(...)?;  // Captura relaciones
    let symbolic = logic_rules.apply(attended);   // Razonamiento simbÃ³lico
    neural_decoder.forward(symbolic)              // Decode a output
}
```

### 3. Cross-Attention entre Neural y Symbolic
```rust
// Neural features como Query
// Symbolic concepts como Key/Value
let cross_attn = MultiHeadAttention::new(512, 8, 0.1)?;
let (output, weights) = cross_attn.forward(
    neural_features,    // Q: features de red neuronal
    symbolic_concepts,  // K: conceptos simbÃ³licos
    symbolic_concepts,  // V: conceptos simbÃ³licos
    ...
)?;
// Permite que red neuronal "atienda" a conocimiento simbÃ³lico
```

---

## ğŸ“ ComparaciÃ³n con PyTorch

### PyTorch:
```python
import torch.nn as nn

# Multi-head attention en PyTorch
mha = nn.MultiheadAttention(embed_dim=512, num_heads=8)
output, weights = mha(query, key, value, attn_mask=mask)
```

### Charl:
```rust
use charl::attention::MultiHeadAttention;

// Multi-head attention en Charl
let mha = MultiHeadAttention::new(512, 8, 0.1)?;
let (output, weights) = mha.forward(
    query, key, value,
    query_shape, key_shape, value_shape,
    Some(mask)
)?;
```

**Diferencias clave:**
- âœ… Charl es mÃ¡s explÃ­cito con shapes (mÃ¡s control)
- âœ… Charl tiene ownership y lifetime safety (Rust)
- âœ… Charl soporta diferentes seq_len desde el diseÃ±o
- âŒ Charl aÃºn no tiene GPU kernels optimizados (TODO)
- âŒ Charl aÃºn no tiene dropout activo (TODO: training flag)

---

## ğŸ“ TODOs para el Futuro

### Optimizaciones:
1. **GPU Kernels** - Implementar versiones GPU de matrix mul
2. **Flash Attention** - Implementar Flash Attention para O(n) memory
3. **Sparse Attention** - Attention patterns sparse para sequences largas
4. **Quantized Attention** - INT8 attention para inferencia

### Features Adicionales:
5. **Relative Position Encoding** - Encoding relativo de posiciones
6. **ALiBi** - Attention with Linear Biases (sin positional encoding)
7. **Rotary Position Embedding (RoPE)** - Como en LLaMA
8. **Grouped Query Attention** - Como en LLaMA 2

### Entrenamiento:
9. **Dropout funcional** - Agregar flag de training/eval
10. **Gradient Checkpointing** - Para sequences muy largas
11. **Xavier/He Initialization** - InicializaciÃ³n proper de pesos

---

## ğŸ¯ PrÃ³ximos Pasos

### âœ… Completado:
- Scaled Dot-Product Attention
- Self-Attention
- Multi-Head Attention
- Layer Normalization
- Configuraciones y tests

### â­ï¸ Saltado (no crÃ­tico para Fase 14):
- Positional Encoding (knowledge graphs no necesitan orden)
- Transformer Block completo (lo construimos on-demand)

### ğŸš€ Siguiente:
**FASE 14: NEURO-SYMBOLIC INTEGRATION**

Con los mecanismos de attention listos, tenemos la base para:
1. **Graph Neural Networks** - Attention sobre grafos de conocimiento
2. **Hybrid Layers** - IntegraciÃ³n neural-symbolic
3. **Cross-Attention** - Entre features neuronales y conceptos simbÃ³licos
4. **Relational Reasoning** - Capturar relaciones entre entidades

---

## ğŸ’¡ Lecciones Aprendidas

### 1. **ValidaciÃ³n de Dimensiones es Crucial**
```rust
// Bug inicial: req seq_len_q == seq_len_k
// Fix: Solo req seq_len_k == seq_len_v
```

### 2. **Numerical Stability en Softmax**
```rust
// Sin max subtraction: overflow en exp()
// Con max subtraction: estable para valores grandes
```

### 3. **Multi-Head requiere Book-keeping Cuidadoso**
```rust
// Split â†’ Process â†’ Concat â†’ Project
// Cada paso tiene diferentes shapes
// Tests ayudaron a catch bugs
```

### 4. **Tests Incrementales son Clave**
```rust
// Test cada componente por separado:
// 1. MatMul â†’ OK
// 2. Softmax â†’ OK
// 3. Attention â†’ OK
// 4. Multi-Head â†’ OK
```

---

## ğŸ“š Referencias

### Papers Implementados:
1. **"Attention Is All You Need"** (Vaswani et al., 2017)
   - Scaled Dot-Product Attention
   - Multi-Head Attention
   - Transformer architecture

### Para Fase 14:
2. **"Graph Attention Networks"** (VeliÄkoviÄ‡ et al., 2018)
   - Attention sobre grafos
3. **"Neural-Symbolic Learning and Reasoning"** (Garcez et al., 2015)
   - IntegraciÃ³n neural-symbolic

---

## ğŸ‰ Logros

### CÃ³digo:
- âœ… 1,628 lÃ­neas de attention mechanisms
- âœ… 34 tests comprehensivos (100% passing)
- âœ… 4 componentes principales funcionando
- âœ… Soporte para encoder-decoder attention
- âœ… Layer Normalization para estabilidad

### Arquitectura:
- âœ… DiseÃ±o modular y extensible
- âœ… Configuraciones flexibles
- âœ… API limpia y type-safe
- âœ… Numerical stability integrada

### PreparaciÃ³n:
- âœ… **Base sÃ³lida para Fase 14 (Neuro-Symbolic)**
- âœ… Attention listo para Knowledge Graphs
- âœ… Layer Norm para deep hybrid networks
- âœ… Componentes probados y documentados

---

**"De operaciones simples a razonamiento complejo."**

**Charl: Attention Mechanisms listos para Neuro-Symbolic AI. ğŸ§ âš¡**

---

**Fecha:** 2025-11-04
**Estado:** âœ… Completado
**PrÃ³ximo:** Fase 14 - Neuro-Symbolic Integration
