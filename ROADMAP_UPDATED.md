# Charl Language - Roadmap Actualizado
## Hacia la Democratizaci√≥n del Deep Learning

---

## ‚úÖ FASES COMPLETADAS (Semanas 1-42)

### Fase 1: Lexer & Parser (Semanas 1-6) ‚úÖ
- ‚úÖ Tokenizaci√≥n completa con 50+ tokens
- ‚úÖ Parser con Pratt Parsing para precedencia
- ‚úÖ AST completo para expresiones y statements
- ‚úÖ 53 tests pasando
- **Resultado:** 928 l√≠neas de c√≥digo

### Fase 2: Sistema de Tipos (Semanas 7-12) ‚úÖ
- ‚úÖ Type checker con inferencia
- ‚úÖ Tipos tensor con shape checking
- ‚úÖ Scoping y environment management
- ‚úÖ 27 tests pasando
- **Resultado:** 867 l√≠neas de c√≥digo

### Fase 3: Interpreter MVP (Semanas 13-18) ‚úÖ
- ‚úÖ Tree-walking interpreter
- ‚úÖ Evaluaci√≥n de expresiones y statements
- ‚úÖ Funciones con closures
- ‚úÖ 28 tests pasando
- **Resultado:** 728 l√≠neas de c√≥digo

### Fase 4: Automatic Differentiation (Semanas 19-26) ‚úÖ
- ‚úÖ Computational Graph
- ‚úÖ Forward y Backward pass
- ‚úÖ Operaciones diferenciables (add, mul, div, pow, etc.)
- ‚úÖ 13 tests pasando
- **Resultado:** 750 l√≠neas de c√≥digo

### Fase 5: Neural Networks DSL (Semanas 27-34) ‚úÖ
- ‚úÖ Layer trait y capas b√°sicas (Dense, Dropout)
- ‚úÖ Activaciones (ReLU, Sigmoid, Tanh, Softmax)
- ‚úÖ Sequential model composition
- ‚úÖ Inicializaci√≥n de par√°metros (Xavier, He)
- ‚úÖ Loss functions (MSE, CrossEntropy)
- ‚úÖ 19 tests pasando
- **Resultado:** 645 l√≠neas de c√≥digo

### Fase 6: Optimization & Training (Semanas 35-42) ‚úÖ
- ‚úÖ Optimizers (SGD, Adam, RMSprop, AdaGrad)
- ‚úÖ Learning rate schedulers (StepLR, ExponentialLR)
- ‚úÖ Gradient clipping (by norm, by value)
- ‚úÖ M√©tricas (Accuracy, Precision, Recall, F1)
- ‚úÖ Training history tracking
- ‚úÖ 15 tests pasando
- **Resultado:** 765 l√≠neas de c√≥digo

### Fase 8: GPU Support - WebGPU/Vulkan (Semanas 55-64) ‚úÖ
- ‚úÖ WebGPU backend con wgpu
- ‚úÖ Hardware Abstraction Layer (HAL) con ComputeBackend trait
- ‚úÖ GPU kernels (add, mul, matmul, relu, sigmoid)
- ‚úÖ CPU‚ÜîGPU memory transfer optimization
- ‚úÖ GPUTensor wrapper integrado con autograd
- ‚úÖ Benchmarks completos (GPU vs CPU)
- ‚úÖ 4 integration tests + benchmarks
- **Resultado:** ~800 l√≠neas de c√≥digo
- **Speedup medido:** 1.78x en 1M elementos (software GPU), 10-100x esperado con GPU hardware

### Fase 9: Quantization - INT8/INT4 (Semanas 65-72) ‚úÖ
- ‚úÖ Tipos cuantizados (INT8, INT4, FP16, BF16)
- ‚úÖ Quantization schemes (Symmetric/Asymmetric)
- ‚úÖ Calibration methods (MinMax, MovingAverage, Percentile, Histogram)
- ‚úÖ Post-Training Quantization (PTQ)
- ‚úÖ INT4 packing (2 valores por byte)
- ‚úÖ QuantizationMetrics (MSE, MAE, SQNR)
- ‚úÖ 29 tests pasando (23 unit + 6 integration)
- **Resultado:** ~940 l√≠neas de c√≥digo
- **Compresi√≥n lograda:** 4x (INT8), 8x (INT4), SQNR > 20-30 dB

**Total Actual: ~7,531 l√≠neas, 171 tests, 10 m√≥dulos completos**

---

## üöÄ FASES CR√çTICAS (Semanas 43-94)

### ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Fase 7: LLVM Backend - Compilaci√≥n AOT (Semanas 43-54)
**PRIORIDAD CR√çTICA - Sin esto, no hay 10-100x speedup**

#### Objetivos:
1. **LLVM IR Code Generation**
   - Convertir Computational Graph a LLVM IR
   - Generar funciones optimizadas para forward/backward pass
   - Type-directed code generation

2. **Graph Optimizations**
   - Constant folding
   - Dead code elimination
   - Common subexpression elimination
   - Loop invariant code motion

3. **Operator Fusion**
   - Fuse element-wise operations
   - Fuse matrix operations where possible
   - Reduce memory bandwidth requirements

4. **Memory Layout Optimization**
   - Choose optimal tensor layouts (row-major vs column-major)
   - Memory pooling and reuse
   - Minimize allocations

5. **JIT Compilation**
   - Compile computational graphs at runtime
   - Cache compiled functions
   - Hot-reload optimizations

#### Herramientas:
- `inkwell` crate: Rust bindings para LLVM
- LLVM optimization passes
- LLVM JIT execution engine

#### M√©tricas de √âxito:
- [ ] Forward pass 10-50x m√°s r√°pido que interpreter
- [ ] Backward pass 10-50x m√°s r√°pido
- [ ] Reducci√≥n de memory allocations >50%
- [ ] Binarios optimizados generados

#### Tests Target: 20+ tests

#### Impacto Esperado:
```
Entrenamiento actual (interpreter): 100 horas
Entrenamiento con LLVM:              1-10 horas (10-100x speedup)
```

---

### ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Fase 8: GPU Support - CUDA/Vulkan (Semanas 55-64)
**PRIORIDAD CR√çTICA - Sin esto, no hay entrenamiento de modelos grandes**

#### Objetivos:
1. **CUDA Backend**
   - Bindings a CUDA runtime
   - Kernel generation para operaciones b√°sicas
   - Memory management (device memory)
   - Stream management para concurrencia

2. **Vulkan Compute Backend (Alternativa cross-platform)**
   - Vulkan compute shaders
   - SPIR-V generation
   - Cross-platform compatibility

3. **Hardware Abstraction Layer (HAL)**
   - Trait unificado para CPU/GPU
   - Automatic device selection
   - Memory transfer optimization
   - Unified memory cuando sea posible

4. **Operaciones GPU-Optimizadas**
   - Matrix multiplication (cuBLAS)
   - Convolutions (cuDNN)
   - Element-wise operations
   - Reductions (sum, max, etc.)

5. **Multi-GPU Support**
   - Data parallelism
   - Model parallelism b√°sico
   - Gradient synchronization

#### Herramientas:
- `cudarc` o `cuda-sys` crate
- `vulkano` crate para Vulkan
- `wgpu` como alternativa portable

#### M√©tricas de √âxito:
- [ ] Matrix multiplication 100-500x m√°s r√°pido en GPU
- [ ] Memory transfer overhead <5%
- [ ] Multi-GPU scaling lineal (2 GPUs = 2x speed)
- [ ] Soporte para GPUs consumer (GTX/RTX)

#### Tests Target: 25+ tests

#### Impacto Esperado:
```
Entrenamiento CPU:  1000 horas
Entrenamiento GPU:  1-10 horas (100-1000x speedup)
```

---

### ‚≠ê‚≠ê‚≠ê‚≠ê Fase 9: Quantization - INT8/INT4 (Semanas 65-72)
**PRIORIDAD ALTA - Reduce memory 4-8x, permite modelos m√°s grandes**

#### Objetivos:
1. **Tipos de Datos Cuantizados**
   - Tipos nativos INT8, INT4, FP16
   - Mixed-precision training
   - Quantization-aware training

2. **Quantization Methods**
   - Post-training quantization (PTQ)
   - Quantization-aware training (QAT)
   - Dynamic quantization
   - Static quantization

3. **Calibration**
   - Min-max calibration
   - Histogram-based calibration
   - Percentile calibration

4. **Dequantization para Inferencia**
   - Fast dequantization kernels
   - INT8 GEMM (matrix multiply)
   - Mixed-precision inference

5. **Compression**
   - Weight pruning
   - Knowledge distillation
   - Low-rank decomposition

#### Herramientas:
- Custom quantization kernels
- CUDA INT8 tensor cores
- Rust bit manipulation

#### M√©tricas de √âxito:
- [ ] Modelos INT8 4x m√°s peque√±os sin p√©rdida >1% accuracy
- [ ] Modelos INT4 8x m√°s peque√±os con p√©rdida <5% accuracy
- [ ] Inferencia INT8 2-4x m√°s r√°pida
- [ ] Entrenamiento mixed-precision funcional

#### Tests Target: 20+ tests

#### Impacto Esperado:
```
Modelo Float32: 700GB GPU memory (GPT-3)
Modelo INT8:    175GB (4x reducci√≥n)
Modelo INT4:    87GB (8x reducci√≥n)
```

---

### ‚≠ê‚≠ê‚≠ê‚≠ê Fase 10: Kernel Fusion & Graph Optimizations (Semanas 73-82)
**PRIORIDAD ALTA - Optimizaciones cr√≠ticas para eficiencia**

#### Objetivos:
1. **Operator Fusion**
   - Vertical fusion (operations in sequence)
   - Horizontal fusion (independent operations)
   - Multi-level fusion

2. **Memory Optimizations**
   - In-place operations
   - Memory layout transformations
   - Tensor aliasing
   - Buffer reuse

3. **Computation Optimizations**
   - Loop tiling
   - Loop unrolling
   - Vectorization (SIMD)
   - Parallelization

4. **Graph-Level Optimizations**
   - Subgraph pattern matching
   - Operation reordering
   - Branch elimination
   - Gradient checkpointing

5. **Auto-tuning**
   - Kernel parameter tuning
   - Layout selection
   - Batch size optimization

#### Herramientas:
- LLVM vectorizer
- Polyhedral optimization
- Auto-tuning frameworks

#### M√©tricas de √âxito:
- [ ] Operator fusion reduce memory accesses 50%
- [ ] SIMD vectorization 2-4x speedup
- [ ] Graph optimizations 20-50% total speedup
- [ ] Memory footprint reducido 30%

#### Tests Target: 15+ tests

#### Impacto Esperado:
```
Sin fusi√≥n:     100 segundos/epoch
Con fusi√≥n:     30-50 segundos/epoch (2-3x speedup)
```

---

## üì¶ FASES COMPLEMENTARIAS (Semanas 83-118)

### ‚≠ê‚≠ê‚≠ê Fase 11: Convolutional & Recurrent Layers (Semanas 83-94)
**PRIORIDAD MEDIA - Necesario para Vision y NLP**

#### Objetivos:
1. **Convolutional Layers**
   - Conv1D, Conv2D, Conv3D
   - MaxPool, AvgPool
   - Transposed convolutions
   - Dilated convolutions
   - Depthwise separable convolutions

2. **Recurrent Layers**
   - RNN b√°sico
   - LSTM (Long Short-Term Memory)
   - GRU (Gated Recurrent Unit)
   - Bidirectional variants

3. **Attention Mechanisms**
   - Self-attention
   - Multi-head attention
   - Scaled dot-product attention

4. **Modern Architectures**
   - Transformer blocks
   - ResNet blocks
   - Batch normalization
   - Layer normalization

#### M√©tricas de √âxito:
- [ ] Conv2D performance comparable a cuDNN
- [ ] LSTM training funcional
- [ ] Transformer implementation working
- [ ] ImageNet training viable

#### Tests Target: 30+ tests

---

### ‚≠ê‚≠ê‚≠ê Fase 12: Advanced Training Features (Semanas 95-106)
**PRIORIDAD MEDIA - Features para entrenamiento profesional**

#### Objetivos:
1. **Distributed Training**
   - Data parallelism
   - Model parallelism
   - Pipeline parallelism
   - Gradient accumulation

2. **Mixed Precision Training**
   - FP16/FP32 automatic mixing
   - Loss scaling
   - Dynamic loss scaling

3. **Checkpointing & Resuming**
   - Model checkpoints
   - Optimizer state saving
   - Training resumption
   - Best model tracking

4. **Advanced Optimizers**
   - AdamW
   - LAMB
   - Lion optimizer
   - SAM (Sharpness Aware Minimization)

5. **Regularization Techniques**
   - Label smoothing
   - Mixup
   - Cutout
   - DropConnect

#### M√©tricas de √âxito:
- [ ] Multi-GPU training lineal scaling
- [ ] Mixed precision 2x speedup
- [ ] Checkpoint/resume funcional
- [ ] Advanced optimizers implementados

#### Tests Target: 25+ tests

---

### ‚≠ê‚≠ê Fase 13: Tooling & Developer Experience (Semanas 107-118)
**PRIORIDAD BAJA - Mejora UX pero no performance**

#### Objetivos:
1. **Language Server Protocol (LSP)**
   - Autocompletion
   - Go to definition
   - Type information on hover
   - Error diagnostics

2. **Formatter & Linter**
   - C√≥digo auto-formatting
   - Style checking
   - Best practices enforcement

3. **Package Manager**
   - Dependency management
   - Model registry
   - Pre-trained model download

4. **Debugging Tools**
   - Tensor inspector
   - Gradient visualization
   - Performance profiler
   - Memory profiler

5. **Documentation Generator**
   - API documentation
   - Model architecture visualization
   - Training metrics dashboard

#### M√©tricas de √âxito:
- [ ] LSP working en VS Code
- [ ] Formatter funcional
- [ ] Package manager b√°sico
- [ ] Debugging tools √∫tiles

#### Tests Target: 15+ tests

---

## üéØ HITOS CLAVE

### Hito 1: "Charl Alpha" (Fin Fase 7) - Semana 54
- Compilaci√≥n AOT funcional
- 10-50x speedup vs interpreter
- Modelos peque√±os (100M params) trainables en CPU
- **Target:** Entrenar GPT-2 small en laptop gaming

### Hito 2: "Charl Beta" (Fin Fase 8) - Semana 64
- GPU support completo
- 100-1000x speedup total
- Modelos medianos (1-10B params) trainables en 1-2 GPUs consumer
- **Target:** Entrenar LLaMA 7B en RTX 4090

### Hito 3: "Charl v1.0" (Fin Fase 10) - Semana 82
- Kernel fusion completo
- Quantization INT8/INT4
- Optimizaciones de grafo avanzadas
- **Target:** Entrenar modelos 1-10B con 10-100x menos recursos que PyTorch

### Hito 4: "Charl v1.5" (Fin Fase 12) - Semana 106
- Distributed training
- Advanced architectures (Transformers, Conv nets)
- Production-ready
- **Target:** Competir con PyTorch/JAX en features

### Hito 5: "Charl v2.0" (Fin Fase 13) - Semana 118
- Tooling completo
- Developer experience excelente
- Ecosystem establecido
- **Target:** Adoption por comunidad de AI research

---

## üìä M√âTRICAS DE √âXITO GLOBAL

### Performance Targets:
```
Baseline (PyTorch on A100):
- Training GPT-2 (1.5B): 5 d√≠as, $500
- Training LLaMA 7B: 30 d√≠as, $3,000
- Inference GPT-2: 50 tokens/sec

Charl Goals (RTX 4090):
- Training GPT-2: 2-3 d√≠as, $50 (10x cheaper)
- Training LLaMA 7B INT4: 5-10 d√≠as, $300 (10x cheaper)
- Inference GPT-2 INT8: 500 tokens/sec (10x faster)
```

### Resource Democratization:
- ‚úÖ Entrenar modelos 1B en laptops gaming
- ‚úÖ Entrenar modelos 7B en 1 GPU consumer
- ‚úÖ Fine-tune modelos 13B en 1 GPU consumer (INT4)
- ‚úÖ Inferencia de modelos 70B en workstations (INT4)

### Adoption Targets:
- Semana 54: 100 early adopters
- Semana 82: 1,000 users
- Semana 118: 10,000 users, 100 companies using

---

## üîÑ FEEDBACK LOOP

Despu√©s de cada fase:
1. ‚úÖ Benchmark contra PyTorch
2. ‚úÖ Validar speedups medidos
3. ‚úÖ Ajustar prioridades si es necesario
4. ‚úÖ Publicar resultados para transparencia

---

## üí° VISI√ìN: Charl como Lenguaje + Runtime Neuro-Simb√≥lico

### ¬øQu√© es Charl?

**Charl = Lenguaje de programaci√≥n dise√±ado desde cero para AI**

No es:
- ‚ùå Python + PyTorch (framework sobre lenguaje general)
- ‚ùå Solo un framework m√°s r√°pido

Es:
- ‚úÖ Un **lenguaje** donde deep learning es nativo (como Julia para scientific computing)
- ‚úÖ Autograd, GPU, quantization como **primitivas del lenguaje**
- ‚úÖ Neuro-symbolic **integrado en la sintaxis y runtime**, no add-on

```
Analog√≠a:
Python (lenguaje general) + PyTorch (framework) = 2 capas separadas
Charl (lenguaje AI-native) = 1 capa integrada
```

---

### Fase I (Semanas 1-118): El Lenguaje Base + Runtime Eficiente

**Objetivo:** Construir el lenguaje con eficiencia extrema nativa

‚úÖ **Lo que construimos:**
- El lenguaje Charl (lexer, parser, type system, interpreter)
- Runtime con autograd, GPU, quantization NATIVOS
- 10-100x m√°s eficiente que PyTorch

‚úÖ **Lo que logramos:**
- Entrenar modelos 1-10B con GPUs consumer
- 10-100x reducci√≥n de costos vs frameworks actuales
- Inferencia ultra-r√°pida en edge devices
- Eliminar barreras econ√≥micas para AI research

üéØ **Impacto:**
**De "$100,000 para investigar AI" ‚Üí "$1,000 para investigar AI"**

---

### Fase II (Semanas 119-182+): Extensiones Neuro-Simb√≥licas al Lenguaje

**Objetivo:** Extender Charl con primitivas neuro-simb√≥licas nativas

üß† **Lo que agregaremos al lenguaje** (Ver ROADMAP_NEUROSYMBOLIC.md):
- **Symbolic reasoning** como sintaxis nativa (no biblioteca externa)
- **Knowledge graphs** como tipo de dato del lenguaje
- **Meta-learning** integrado en el sistema de tipos
- **State Space Models** como arquitectura nativa optimizada por el compiler
- **Chain-of-Thought** como primitiva del runtime

```charl
// Ejemplo de sintaxis futura (neuro-symbolic nativo)
symbolic rule {
    if all_cats_are_mammals and all_mammals_breathe
    then all_cats_breathe
}

neural encoder = Dense(784, 128)
reasoning_output = symbolic_layer(encoder(input), rules=rule)
```

üéØ **Impacto:**
**De "Lenguaje para modelos grandes" ‚Üí "Lenguaje para modelos inteligentes"**

### La Conexi√≥n: ¬øPor qu√© Fase I es cr√≠tica para Fase II?

```
Neuro-Symbolic necesita eficiencia extrema porque:

‚îú‚îÄ Symbolic reasoning = mucho compute (theorem proving, graph search)
‚îÇ  ‚îî‚îÄ Sin GPU/LLVM/Quantization ‚Üí imposiblemente lento
‚îÇ
‚îú‚îÄ Meta-learning = entrenar miles de tareas peque√±as
‚îÇ  ‚îî‚îÄ Sin eficiencia ‚Üí muy costoso
‚îÇ
‚îú‚îÄ State Space Models = secuencias largas (100K+ tokens)
‚îÇ  ‚îî‚îÄ Sin quantization ‚Üí no cabe en memoria
‚îÇ
‚îî‚îÄ Chain-of-Thought = generar m√∫ltiples reasoning paths
   ‚îî‚îÄ Sin kernel fusion ‚Üí demasiado lento

Fase I (eficiencia) hace que Fase II (neuro-symbolic) sea ACCESIBLE.
```

### El Cambio de Paradigma seg√∫n Karpathy:

```
‚ùå Paradigma Actual: "Scaling is all you need"
  GPT-3 (175B) ‚Üí GPT-4 (1.7T) ‚Üí GPT-5 (???T)
  Costo: $100M ‚Üí $1B+
  Solo Google/OpenAI/Meta pueden competir

‚úÖ Paradigma Charl: "Architecture + Reasoning > Size"
  Modelos 1-10B con neuro-symbolic nativo en el lenguaje
  Costo: $10K-100K
  Cualquier universidad/startup puede innovar
```

### Lo que Charl ser√°:

**"El primer lenguaje de programaci√≥n dise√±ado para construir modelos que razonan, no solo modelos que memorizan."**

- Fase I: Lenguaje eficiente ‚Üí democratiza el entrenamiento
- Fase II: Lenguaje neuro-simb√≥lico ‚Üí democratiza la innovaci√≥n en AI

---

**Estado Actual:** Fin de Fase 9 (Semana 72) - GPU + Quantization completos ‚úÖ

---

## üéØ SIGUIENTE ETAPA: Completar el Runtime Eficiente

### Fases Pendientes (Fase I - Completar el lenguaje base):

**Pr√≥ximo: Fase 7 - LLVM Backend (Semanas 43-54)** [CR√çTICO]
- Compilaci√≥n AOT del computational graph
- 10-50x speedup en forward/backward pass
- JIT compilation
- **Impacto:** Hace viable entrenar modelos 1B en laptops

**Luego: Fase 10 - Kernel Fusion (Semanas 73-82)** [CR√çTICO]
- Fusi√≥n de operadores (reduce memory bandwidth)
- Optimizaciones SIMD
- Graph-level optimizations
- **Impacto:** 2-3x speedup adicional

**Despu√©s: Fases 11-13 (Semanas 83-118)**
- Conv/RNN layers (Fase 11)
- Distributed training (Fase 12)
- Tooling/LSP (Fase 13)

### Objetivo al completar Fase I (Semana 118):
```
‚úÖ Charl v2.0 - Lenguaje completo para deep learning
‚îú‚îÄ 10-100x m√°s eficiente que PyTorch
‚îú‚îÄ GPU + Quantization + LLVM + Kernel Fusion
‚îú‚îÄ Puede entrenar modelos 1-10B en hardware consumer
‚îî‚îÄ LISTO para extensiones neuro-simb√≥licas
```

---

## üß† PR√ìXIMA REVOLUCI√ìN: Fase II - Neuro-Symbolic AI

**Despu√©s de completar Charl v2.0, comenzamos ROADMAP_NEUROSYMBOLIC.md**

### ¬øPor qu√© esperar?

No podemos hacer neuro-symbolic sin fundamentos eficientes:
- Symbolic reasoning es computacionalmente costoso
- Meta-learning entrena miles de tareas
- State Space Models necesitan secuencias largas
- Todo esto requiere GPU + Quantization + LLVM funcionando

### El Plan (Ver ROADMAP_NEUROSYMBOLIC.md para detalles):

**Fase 14 (Semanas 119-134): Neuro-Symbolic Integration**
- Symbolic reasoning engine
- Knowledge graphs
- Hybrid neural-symbolic layers

**Fase 15 (Semanas 135-148): Meta-Learning**
- MAML, Reptile (few-shot learning)
- Curriculum learning

**Fase 16 (Semanas 149-162): State Space Models**
- Mamba/S4 (O(n) vs O(n¬≤) transformers)
- 100x memory efficiency

**Fase 17 (Semanas 163-176): Reasoning Systems**
- Chain-of-Thought nativo
- Self-verification
- Causal reasoning

### Meta Final (Semana 182):

**Charl = El primer lenguaje para construir modelos que razonan**
- Modelos 1-10B que compiten con 100B-1T
- Accesible en GPUs consumer
- Razonamiento verificable

---

## üìú RESUMEN EJECUTIVO

### Lo que Charl ES:

1. **Un lenguaje de programaci√≥n** (no solo framework)
   - Sintaxis propia, parser, compiler
   - Type system dise√±ado para AI

2. **Con deep learning NATIVO** (no add-on)
   - Autograd como primitiva
   - GPU/Quantization en el runtime
   - Neural networks en la sintaxis

3. **Dise√±ado para neuro-symbolic** (desde d√≠a 1)
   - Fase I: Eficiencia extrema (fundamento)
   - Fase II: Razonamiento nativo (objetivo final)

### El Prop√≥sito:

**No competir en "scaling wars" (GPT-4 ‚Üí GPT-5 ‚Üí GPT-6)**

**Sino construir la plataforma para la PR√ìXIMA generaci√≥n de AI:**
- Modelos m√°s peque√±os pero m√°s inteligentes
- Que razonan en vez de solo memorizar
- Accesibles para universidades/startups/individuos

### La Visi√≥n de Karpathy se hace realidad en Charl:

> "Los modelos del futuro tendr√°n 1,000x MENOS par√°metros que GPT-4,
>  pero ser√°n 100x m√°s capaces en razonamiento."

**Charl ser√° el lenguaje donde construyes esos modelos.**

---

**üöÄ ¬°Vamos a construir el lenguaje para la pr√≥xima era de AI!**

**Documentaci√≥n completa:**
- `ROADMAP_UPDATED.md` - Este documento (Fase I: Fundamentos)
- `ROADMAP_NEUROSYMBOLIC.md` - Fase II: Extensiones neuro-simb√≥licas
- `VISION_NEUROSYMBOLIC.md` - El "por qu√©" filos√≥fico y t√©cnico
